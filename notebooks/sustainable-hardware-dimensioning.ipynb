{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd2dc3d-98f9-4c62-861e-e278f6d1e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Notebook setup: run this before everything\n",
    "# ============================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Control figure size\n",
    "interactive_figures = False\n",
    "if interactive_figures:\n",
    "    # Normal behavior\n",
    "    %matplotlib widget\n",
    "    figsize=(9, 3)\n",
    "else:\n",
    "    # PDF export behavior\n",
    "    figsize=(14, 5)\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "from util import util\n",
    "#from scipy.integrate import odeint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from skopt.space import Space\n",
    "#from eml.net.reader import keras_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a91681-7e62-40a9-b6b8-8d641791d19b",
   "metadata": {},
   "source": [
    "# Case study: Sustainable Hardware Dimensioning\n",
    "\n",
    "With widely recognised power-hungry and expensive training algorithms, deep learning has begun to address its carbon footprint. Machine learning (ML) models have grown exponentially in size over the past few years, with some algorithms training for thousands of core-hours, and the associated energy consumption and cost have become a growing concern [Green AI paper]. \n",
    "\n",
    "Previous studies have made advances in estimating GHG emissions of computation, and have attempted in providing general and easy-to-use methodologies for estimating carbon footprint that can be applied to any computational task [Green Algorithms paper].\n",
    "\n",
    "In this work, we explore the dimension of finding the the best Hardware architecture and its dimensioning for AI algorithms, while respecting constraints in terms of carbon emissions. Previous work [HADA paper] has focused on HW Dimensioning for AI algorithms with constraints on budget, time and solution quality. This problem is called Hardware Dimensioning. In this work, we aim at extending this approach by also considering constraints on carbon emissions of the computations, and we name this problem Sustainable Hardware Dimensioning.\n",
    "\n",
    "The HADA approach is based on the Empirical Model Learning paradigm [EML paper], which integrates Machine Learning (ML) models into an optimisation problem. The key idea is to integrate domain knowledge held by experts with data-driven models that learn the relationships between HW requirements and AI algorithm performances, which would be very complex to express formally in a suitable model. The approach starts with benchmarking multiple AI algorithms on different HW resources, generating data used to train ML models; then, optimisation is used to find the best [HW configuration](https://www.sciencedirect.com/topics/computer-science/hardware-configuration) that respects user-defined constraints.\n",
    "\n",
    "# Methodology\n",
    "\n",
    "At the basis of our approach is the Empirical Model Learning (EML) paradigm. Broadly speaking, EML deals with solving declarative optimisation models with a complex component $h$, which represents the relation between variables which can be acted upon $x$ (the decision variables) and the observables related to the system considered; the function $h(x) = y$ describes this relationships. As the $h(x)$ is complex, we cannot optimise directly over it. Hence, we exploit empirical knowledge to build a surrogate model $h_\\theta(x)$ learned from data, where $\\theta$ is the parameter vector.\n",
    "\n",
    "HADA (HArdware Dimensioning of AI Algorithms), is then constituted of three main phases\n",
    "\n",
    "1. data set collection (benchmarking phase) - an initial phase to collect the data set by running multiple times the target algorithms, under different configurations;\n",
    "2. surrogate model creation - once a training set is available, a set of ML models is then trained on such data and then these models are encoded as a set of variables and constraints following EML paradigm;\n",
    "3. optimisation – post the user-defined constraints and objective function on top of the combinatorial structure formed by the encoded ML models and the domain-knowledge constraints, and finally solve the optimisation model (either until an optimal solution or a time limit is reached.\n",
    "\n",
    "## Dataset Collection\n",
    "\n",
    "We built a training set based on grounding the two stochastic algorithms, i.e., anticipate and contingency [32, 4] from the energy management system domain. The two algorithms calculate the amount of energy that must be produced by the energy system to meet the required load, minimising the total energy cost over the daily time horizon and by taking into account the uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f2df48-e64c-463c-868c-be035158ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration: look at the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302f4f4-0472-48c8-b1b4-1c3e96faaecd",
   "metadata": {},
   "source": [
    "### Measuring Carbon Emissions\n",
    "\n",
    "In order to extend HADA for taking into account sustainability, we need to measure the carbon emissions for running the algorithms during the benchmark phase. A simple tool to do so is [codecarbon](https://mlco2.github.io/codecarbon/index.html), which is a python package offering useful tools for tracking the emissions resulting from executing code execution.\n",
    "\n",
    "The CO2e emission tracking tool offered by codecarbon can be used in [different modalities](https://mlco2.github.io/codecarbon/usage.html): as an Explicit Object, as a Context Manager (recommended for monitoring a specific code block) or as a Decorator (recommended for monitoring training functions). To monitor the emissions for the execution of the ANTICIPATE and CONTINGENCY algorithms over a single instance, i created an explicit `EmissionTracker` and used the `start()` and `stop()` functions to respectively start and stop the tracking of the code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ca68ef-3fb4-4cb1-971b-08ca46795d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking of the algorithms emissions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
